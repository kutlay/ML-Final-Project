{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   job_id salary_range  telecommuting  has_company_logo  has_questions  \\\n",
      "0       1                           0                 1              0   \n",
      "1       2                           0                 1              0   \n",
      "2       3                           0                 1              0   \n",
      "3       4                           0                 1              0   \n",
      "4       5                           0                 1              1   \n",
      "\n",
      "   fraudulent                                               text  \n",
      "0           0  Marketing Intern US, NY, New York Marketing We...  \n",
      "1           0  Customer Service - Cloud Video Production NZ, ...  \n",
      "2           0  Commissioning Machinery Assistant (CMA) US, IA...  \n",
      "3           0  Account Executive - Washington DC US, DC, Wash...  \n",
      "4           0  Bill Review Manager US, FL, Fort Worth   SpotS...  \n",
      "\n",
      "\n",
      "Total # of job descriptions:\n",
      "17880\n",
      "\n",
      "\n",
      "Percentage of fraudulent job descriptions:\n",
      "4.8434004474272925\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "# dynamically grow GPU memory \n",
    "config.gpu_options.allow_growth = True \n",
    "set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "import re,string,unicodedata\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "df = pd.read_csv(\"fake_job_postings.csv\")\n",
    "df.fillna(\" \",inplace = True)\n",
    "df['text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function'] \n",
    "\n",
    "del df['title']\n",
    "del df['location']\n",
    "del df['department']\n",
    "del df['company_profile']\n",
    "del df['description']\n",
    "del df['requirements']\n",
    "del df['benefits']\n",
    "del df['employment_type']\n",
    "del df['required_experience']\n",
    "del df['required_education']\n",
    "del df['industry']\n",
    "del df['function']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\\nTotal # of job descriptions:\")\n",
    "print(df.shape[0])\n",
    "print(\"\\n\\nPercentage of fraudulent job descriptions:\")\n",
    "print(df[df.fraudulent == 1].shape[0]/(df[df.fraudulent == 0].shape[0]+df[df.fraudulent == 1].shape[0])*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            pos = pos_tag([i.strip()])\n",
    "            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n",
    "            final_text.append(word.lower())\n",
    "    return \" \".join(final_text)     \n",
    "\n",
    "df.text = df.text.apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df.text, df.fraudulent, test_size=0.3, random_state=2020)\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(x_train)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(x_test)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(x_train)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(x_test)\n",
    "\n",
    "tv_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units = 100 , activation = 'relu' , input_dim = tv_train_reviews.shape[1]))\n",
    "model.add(Dense(units = 50 , activation = 'relu'))\n",
    "model.add(Dense(units = 25 , activation = 'relu'))\n",
    "model.add(Dense(units = 10 , activation = 'relu'))\n",
    "model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(tv_train_reviews,y_train , epochs = 5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model.predict(cv_test_reviews.todense()),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
